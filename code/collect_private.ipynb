{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect PLOS2016 - Facebook Engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import configparser\n",
    "import json\n",
    "import math\n",
    "import requests\n",
    "import queue\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from ratelimit import limits, RateLimitException, sleep_and_retry\n",
    "from facebook import GraphAPI, GraphAPIError\n",
    "\n",
    "try: \n",
    "    # for notebook\n",
    "    get_ipython\n",
    "    from tqdm._tqdm_notebook import tqdm_notebook as tqdm  \n",
    "except: \n",
    "    # for commandline\n",
    "    from tqdm import tqdm \n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = \"../data/plos2016.csv\"\n",
    "urls_csv = \"../data/urls.csv\"\n",
    "query_csv = \"../data/queries.csv\"\n",
    "og_csv = \"../data/og_objects.csv\"\n",
    "\n",
    "config_file = \"../config.cnf\"\n",
    "\n",
    "batchsize = 50\n",
    "sample_size = None\n",
    "continue_crawl = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "Config = configparser.ConfigParser()\n",
    "Config.read(config_file)\n",
    "\n",
    "FACEBOOK_APP_ID = Config.get('facebook', 'app_id')\n",
    "FACEBOOK_APP_SECRET = Config.get('facebook', 'app_secret')\n",
    "FACEBOOK_USER_TOKEN = Config.get('facebook', 'user_token')\n",
    "\n",
    "RATELIMIT = int(Config.get('ratelimit', 'period'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Load FB credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_app_access(app_id, app_secret, version=\"2.10\"):\n",
    "    \"\"\"Exchange a short-lived user token for a long-lived one\"\"\"\n",
    "    payload = {'grant_type': 'client_credentials',\n",
    "               'client_id': app_id,\n",
    "               'client_secret': app_secret}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            'https://graph.facebook.com/oauth/access_token?', params=payload)\n",
    "    except requests.exceptions.RequestException:\n",
    "        raise Exception()\n",
    "\n",
    "    token = json.loads(response.text)\n",
    "    token['created'] = str(datetime.datetime.now())\n",
    "    return token\n",
    "\n",
    "\n",
    "def extend_user_access(user_token, app_id, app_secret, version=\"2.10\"):\n",
    "    \"\"\"Uses a short-lived user token to create a long lived one\"\"\"\n",
    "    payload = {'grant_type': 'fb_exchange_token',\n",
    "               'client_id': app_id,\n",
    "               'client_secret': app_secret,\n",
    "               'fb_exchange_token': user_token}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            'https://graph.facebook.com/oauth/access_token?', params=payload)\n",
    "    except requests.exceptions.RequestException:\n",
    "        raise Exception()\n",
    "\n",
    "    token = json.loads(response.text)\n",
    "    token['created'] = str(datetime.datetime.now())\n",
    "    return token\n",
    "\n",
    "\n",
    "def token_expiry(token):\n",
    "    remain = datetime.timedelta(seconds=token['expires_in'])\n",
    "    created = datetime.datetime.strptime(token['created'], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    print(\"Token expires {}\\n{} left\".format(str(created+remain), str(remain)))\n",
    "\n",
    "\n",
    "def expires_soon(token, tolerance=1):\n",
    "    remain = datetime.timedelta(seconds=token['expires_in'])\n",
    "    created = datetime.datetime.strptime(token['created'], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    if (now - created+remain).days < tolerance:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pickled token\n",
      "Saving token\n",
      "Token expires 2018-09-06 18:44:28.689354\n",
      "59 days, 23:52:41 left\n"
     ]
    }
   ],
   "source": [
    "# fb_graph, token = get_app_access(FACEBOOK_APP_ID, FACEBOOK_APP_SECRET)\n",
    "try:\n",
    "    with open(\"token.pkl\", \"rb\") as pkl:\n",
    "        token = pickle.load(pkl)\n",
    "        print(\"Found pickled token\")\n",
    "    \n",
    "    if expires_soon(token):\n",
    "        token = extend_user_access(FACEBOOK_USER_TOKEN, FACEBOOK_APP_ID, FACEBOOK_APP_SECRET)\n",
    "        print(\"Created new token, because of soon expiry\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No token found. Creating new one...\")\n",
    "    token = extend_user_access(FACEBOOK_USER_TOKEN, FACEBOOK_APP_ID, FACEBOOK_APP_SECRET)\n",
    "    \n",
    "print(\"Saving token\")\n",
    "token_expiry(token)\n",
    "with open(\"token.pkl\", \"wb\") as pkl:\n",
    "    pickle.dump(token, pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_graph = GraphAPI(token['access_token'], version=\"2.10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Load and prepare URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(urls_csv, index_col=\"url_id\")\n",
    "urls = raw\n",
    "\n",
    "query_columns = [\"url_id\", \"error_msg\", \"queried_at\"]\n",
    "queries = pd.DataFrame(columns=query_columns)\n",
    "\n",
    "if continue_crawl:\n",
    "    queries = pd.read_csv(query_csv, index_col=\"query_id\")\n",
    "    urls = urls.drop(queries[old_queries.error_msg.isnull()].url_id)\n",
    "    \n",
    "if sample_size:\n",
    "    urls = urls.sample(sample_size)\n",
    "    \n",
    "    \n",
    "og_columns = [\"og_id\", \"query_id\", \"received_at\", \"fb_url\",\n",
    "              \"og_description\", \"og_title\", \"og_type\", \"og_updated_time\",\n",
    "              \"reactions\", \"shares\", \"comments\", \"plugin_comments\"]\n",
    "og_objects = pd.DataFrame(columns=og_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Run queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_result(url_id, result, queries, og_objects, query_f, og_f, now):\n",
    "    query_id = queries.shape[0]\n",
    "    queries.loc[query_id] = [url_id, result['err_msg'], str(now)]\n",
    "    \n",
    "    query_f.writerow([query_id, url_id, result['err_msg'], str(now)])\n",
    "                             \n",
    "    # if result, record og object\n",
    "    if 'og_obj' in result:\n",
    "        i = og_objects.shape[0]\n",
    "\n",
    "        og_id = result['og_obj']['id']\n",
    "        reactions = int(result['og_eng']['reaction_count'])\n",
    "        shares = int(result['og_eng']['share_count'])\n",
    "        comments = int(result['og_eng']['comment_count'])\n",
    "        plugin_comments = int(result['og_eng']['comment_plugin_count'])\n",
    "\n",
    "        for field in ['description', 'title', 'type', 'updated_time']:\n",
    "            try:\n",
    "                og_objects.loc[i, \"og_{}\".format(field)] = result['og_obj'][field]\n",
    "            except:\n",
    "                og_objects.loc[i, \"og_{}\".format(field)] = None\n",
    "        \n",
    "        og_objects.loc[i, \"fb_url\"] = result[\"fb_url\"]\n",
    "        og_objects.loc[i, \"og_id\"] = og_id\n",
    "        og_objects.loc[i, \"query_id\"] = query_id\n",
    "        og_objects.loc[i, \"received_at\"] = str(result['received'])\n",
    "        og_objects.loc[i, [\"reactions\", \"shares\", \"comments\", \"plugin_comments\"]] = [reactions, shares, comments, plugin_comments]\n",
    "         \n",
    "        og_f.writerow(og_objects.loc[i][og_columns].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_url(url):\n",
    "    result = {}\n",
    "    try:\n",
    "        r = fb_graph.get_object(id=url.strip(), fields=\"engagement,og_object\")\n",
    "    except Exception as e:\n",
    "        result['received'] = datetime.datetime.now()\n",
    "        result['err_msg'] = str(e)\n",
    "        print(e)\n",
    "        return result\n",
    "        \n",
    "    result['received'] = datetime.datetime.now()\n",
    "    result['err_msg'] = None\n",
    "    result['fb_url'] = r['id']\n",
    "    \n",
    "    if 'og_object' in r:\n",
    "        result[\"og_obj\"] = r['og_object']\n",
    "        result[\"og_eng\"]  = r['engagement']\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def query_urls(urls):\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        responses = fb_graph.get_objects(\n",
    "            ids=[url.strip() for url in urls],\n",
    "            fields=\"engagement,og_object\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    received = datetime.datetime.now()\n",
    "\n",
    "    for url, r in responses.items():        \n",
    "        result = {}\n",
    "\n",
    "        result['received'] = received\n",
    "        result['err_msg'] = None\n",
    "        result['fb_url'] = r['id']\n",
    "\n",
    "        if 'og_object' in r:\n",
    "            result[\"og_obj\"] = r['og_object']\n",
    "            result[\"og_eng\"]  = r['engagement']\n",
    "\n",
    "        results[url] = result\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sleep_and_retry\n",
    "@limits(calls=1, period=RATELIMIT)\n",
    "def process_url(batch, queries, og_objects, query_f, og_f):\n",
    "    \"\"\"\"\"\"\n",
    "    try:\n",
    "        now = datetime.datetime.now()\n",
    "        result = query_url(batch.url)\n",
    "        process_result(batch.name, result, queries, og_objects, query_f, og_f, now)\n",
    "    except GraphAPIError as e: \n",
    "        query_f.writerow([queries.shape[0], batch.name, e, str(now)])\n",
    "\n",
    "        \n",
    "@sleep_and_retry\n",
    "@limits(calls=1, period=RATELIMIT)\n",
    "def process_batch(batch, queries, og_objects, query_f, og_f, failed_batches):\n",
    "    \"\"\"\"\"\"\n",
    "    try:\n",
    "        now = datetime.datetime.now()\n",
    "        results = query_urls(batch.url.tolist())\n",
    "\n",
    "        # successful batch query\n",
    "        for url, result in results.items():\n",
    "            url_id = batch[batch.url == url].index[0]\n",
    "            process_result(url_id, result, queries, og_objects, query_f, og_f, now) \n",
    "        \n",
    "    # failed batch query\n",
    "    except GraphAPIError as e: \n",
    "        failed_batches.put((e, batch.index))\n",
    "        \n",
    "        # Process failed batches\n",
    "        pbar = tqdm(total=failed_batches.qsize()*batchsize, desc=\"Failed batches\")\n",
    "        while not failed_batches.empty():\n",
    "            e, bad_batch = failed_batches.get()\n",
    "            if len(bad_batch) > 4:\n",
    "                batch_indices = chunker(bad_batch, math.ceil(len(bad_batch)/2))\n",
    "\n",
    "                for batch_ind in batch_indices:\n",
    "                    batch = urls.loc[batch_ind]\n",
    "\n",
    "                    q_len = failed_batches.qsize()\n",
    "                    process_batch(batch, queries, og_objects, query_writer, og_writer, failed_batches)\n",
    "                    if failed_batches.qsize() == q_len:\n",
    "                        pbar.update(len(batch_ind))\n",
    "\n",
    "            else:\n",
    "                for i in bad_batch:\n",
    "                    process_url(urls.loc[i], queries, og_objects, query_writer, og_writer,)\n",
    "                    pbar.update(1)\n",
    "        pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c0cc9710544f87a2f03b4364c7f18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=3683), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if continue_crawl:\n",
    "    write_mode = \"a\"\n",
    "else:\n",
    "    write_mode = \"w\"\n",
    "    \n",
    "with open(query_csv, write_mode) as query_f, open(og_csv, write_mode) as og_f:\n",
    "    query_writer = csv.writer(query_f, delimiter=\",\")\n",
    "    og_writer = csv.writer(og_f, delimiter=\",\")\n",
    "    \n",
    "    # Write column labels \n",
    "    if not continue_crawl:\n",
    "        query_writer.writerow([\"query_id\"] + queries.columns.tolist())\n",
    "        og_writer.writerow(og_objects.columns.tolist())\n",
    "\n",
    "    # Keep track of indices that failed during batchmode\n",
    "    failed_batches = queue.Queue()\n",
    "\n",
    "    # Initialise indices for batches\n",
    "    if len(urls) < batchsize:\n",
    "        batchsize = len(urls)\n",
    "    batch_indices = chunker(urls.index, batchsize)\n",
    "\n",
    "    # Keep appending in batches of 50\n",
    "    for batch_ind in tqdm(batch_indices, total=len(urls)//batchsize, desc=\"Batches\"):\n",
    "        batch = urls.loc[batch_ind] \n",
    "        process_batch(batch, queries, og_objects, query_writer, og_writer, failed_batches)\n",
    "    \n",
    "    # Process failed batches\n",
    "    pbar = tqdm(total=failed_batches.qsize()*batchsize, desc=\"Failed batches\")\n",
    "    while not failed_batches.empty():\n",
    "        e, bad_batch = failed_batches.get()\n",
    "        if len(bad_batch) > 4:\n",
    "            batch_indices = chunker(bad_batch, math.ceil(len(bad_batch)/2))\n",
    "            \n",
    "            for batch_ind in batch_indices:\n",
    "                batch = urls.loc[batch_ind]\n",
    "                \n",
    "                q_len = failed_batches.qsize()\n",
    "                process_batch(batch, queries, og_objects, query_writer, og_writer, failed_batches)\n",
    "                if failed_batches.qsize() == q_len:\n",
    "                    pbar.update(len(batch_ind))\n",
    "                    \n",
    "        else:\n",
    "            for i in bad_batch:\n",
    "                process_url(urls.loc[i], queries, og_objects, query_writer, og_writer,)\n",
    "                pbar.update(1)\n",
    "    pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altmetrics",
   "language": "python",
   "name": "altmetrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
